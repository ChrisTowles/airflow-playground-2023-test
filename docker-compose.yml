version: '3.8'


# Full Example - https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml


# ====================================== AIRFLOW environment VARIABLES =======================================
x-environment: &airflow_environment
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__FERNET_KEY: 'RWoGg8pFVgzWXkdYm2qymU_HZAoAT8Bk2cBQNFNhxcA=' # this is a dummy key, you should generate your own  with
  # python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
  AIRFLOW__LOGGING__LOGGING_LEVEL: 'INFO'

  USE_SMALL_API: 'true'
  BATCH_SIZE: 40000
  BATCH_SIZE_SMALL: 5000
  COUNT_ENDPOINT: count
  MAX_ACTIVE_TIS_PER_DAG: 8

x-depends_on: &airflow-depends-on
  postgres:
    condition: service_healthy

x-airflow-image: &airflow_image apache/airflow:2.5.2-python3.10
# ===========


services:
  postgres:
    container_name: airflow
    image: postgres:13-alpine # postgres:14-alpine had issues when on mac, no idea why
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    volumes:
      - postgres-db:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  initdb_init:
    image: *airflow_image
    depends_on:
      <<: *airflow-depends-on
    environment: *airflow_environment
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create --username airflow --password airflow --firstname firstname --lastname lastname --role Admin --email admin@example.org
        airflow connections delete 'dev_db'
        airflow connections add 'dev_db' \
            --conn-type 'postgres' \
            --conn-login 'airflow' \
            --conn-password 'airflow' \
            --conn-host 'postgres' \
            --conn-port '5432' \
            --conn-schema 'airflow'
        airflow connections delete 'dev_s3'
        airflow connections add 'dev_s3' \
            --conn-json '{
                "conn_type": "aws",
                "extra": { 
                  "aws_access_key_id": "user", 
                  "aws_secret_access_key": "password",
                  "endpoint_url": "http://locals3:9000", 
                  "region_name": "us-east-1"
                }
            }'

    # tty: true # for debugging purposes

  webserver:
    image: *airflow_image
    container_name: airflow-webserver
    env_file:
      - .env
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    volumes:
      - logs:/opt/airflow/logs
      #- ./airflow-logs:/opt/airflow/logs # for debugging purposes
    ports:
      - "8080:8080"
    environment: *airflow_environment
    command: webserver
    depends_on:
      <<: *airflow-depends-on
      initdb_init:
        condition: service_completed_successfully

  scheduler:
    # build: .
    image: *airflow_image
    container_name: airflow-scheduler
    restart: unless-stopped
    env_file:
      - .env
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      <<: *airflow-depends-on
      initdb_init:
        condition: service_completed_successfully
    volumes:
      - ./dags:/opt/airflow/dags
      - logs:/opt/airflow/logs
      #- ./airflow-logs:/opt/airflow/logs # for debugging purposes
    environment: *airflow_environment
    command: scheduler


  locals3:
    image: minio/minio
    container_name: s3-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    env_file:
      - ./s3_minio/.env.local
    command: server --console-address ":9001" /data
    volumes:
      - locals3-data:/s3_minio/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  locals3_init:
    build: 
      context: ./s3_minio
      dockerfile: DockerFile
    env_file:
      - ./s3_minio/.env.local
    environment:
      - USER=user
      - COPY_DIR=false
      - INPUT_BUCKETS=datasets
    depends_on:
      - locals3
    entrypoint: /bin/sh ./entrypoint.sh

volumes:
  logs:
  postgres-db:
  locals3-data: